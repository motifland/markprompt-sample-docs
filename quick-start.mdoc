## Quick start

The quickest way to get started is to navigate to the [Markprompt dashboard](https://markprompt.com) and follow the onboarding, which consists of two steps:

- Step 1: Uploading and processing a set of files
- Step 2: Querying the content in a playground

Once you have completed the onboarding, you can expose a prompt, for instance on a website, using our [API endpoints](#api) or our [React](#react) or [Web Component](#web-component).

{% note %}
A good starting point for integrating a prompt on your website is the [Markprompt Starter Template](https://github.com/motifland/markprompt-starter-template), which is a sample Next.js application containing the full code for the prompt component, ready to query your content.
{% /note %}

### Processing your content

When you upload your files, Markprompt will split them into sections. A section is delimited by headings (`#`, `##`, ... in Markdown, `<h1>`, `<h2>`, ... in HTML). For instance, if your content looks as follows:

```md
## Welcome to the Acme docs

Thank you for choosing the Acme Framework...

### What is the Acme Framework?

The Acme Framework is a collection of...

### Getting Started

To get started using the Acme Framework...
```

three sections will be generated, one for each opening heading.

Then, each section is passed to the [OpenAI Embeddings API](https://platform.openai.com/docs/guides/embeddings). This creates a "signature" for the content, in the form of a vector that captures essential traits of your content, and makes it possible to subsequently measure how "close" two pieces of content are. It is using the `text-embedding-ada-002` model.

### Querying your content

Now that your content is indexed, we can query it. It happens in two steps.

#### Finding matching sections

When a user enters a query, say "How do I self-host the database", Markprompt transforms that query into an embedding, exactly like it did with the sections in the previous step. This embedding can then be compared to each of your indexed sections, the goal being to find the sections that are "close" to the question, that is, are likely to contain useful information to answer the question. The embeddings are stored in [Supabase](https://supabase.com/) as a `pgvector`, which provides operations to efficiently compare vectors. This is nicely explained on the Supabase blog: [Storing OpenAI embeddings in Postgres with pgvector](https://supabase.com/blog/openai-embeddings-postgres-vector).

#### Building a prompt with context

Markprompt picks the top-10 closest embeddings, if they exist. Among these 10, it also filters out the ones with too little similarity (to avoid unnecessary noise), and the last ones if too large combined.

The source that created these embeddings is then injected into the following "parent prompt":

```
You are a very enthusiastic company representative
who loves to help people! Given the following sections
from the documentation, answer the question using only that
information, outputted in Markdown format. If you are unsure
and the answer is not explicitly written in the documentation,
say "I don't know".

Context sections:
---
{sections}

Question: "{input prompt}"

Answer (including related code snippets if available):
```

and sent to the [OpenAI Completions API](https://platform.openai.com/docs/guides/completion). By default, the API streams back a response as a [ReadableStream](https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream). The stream is of the form:

```
[path1,path2,...]___START_RESPONSE_STREAM___In order to self-host...
```

More precisely, the response stream is split in two parts, separated by the `___START_RESPONSE_STREAM___` tag:

- The first part of the stream is the list of references that were used to produce the content. Namely the page IDs containing the sections that were used in the final prompt.
- The second part is the streamed response, which is the actual result that the completions endpoint produced as an answer to the input prompt.

Note that if the `stream` flag is set to false, the response is returned as a plain JSON object, as detailed in the [completions API reference](#create-completion).
